gpus: 1
accelerator: "cuda" # use ddp for gpus > 1. Also see PyTorch Lightning documentation on distributed training.
workers: 16 # I recommend tuning this parameter for faster data augmentation processing
dataset_dir: "/data/GZTAN"
dataset: "GTZAN"
task: "clustering"

# train options
seed: 24
batch_size: 64
max_epochs: 50
clustering: True
# clustering: False

# SimCLR model options
projection_dim: 256 # Projection dim. of SimCLR projector

# loss options
k: 0.5
optimizer: "Adam" # or LARS (experimental)
learning_rate: 3.0e-4
weight_decay: 1.0e-6 # "optimized using LARS [...] and weight decay of 10âˆ’6"
temperature: 0.5 # see appendix B.7.: Optimal temperature under different batch sizes

# reload options
upstream_checkpoint_path: "/home/ubuntu/AI/clmr/Unsupervised-Rhythm-Clustering-Embedding/runs/CLMRv2-GTZAN-contrastive/version_0/checkpoints/epoch=24-step=1550.ckpt" 
downstream_checkpoint_path: "/home/ubuntu/AI/clmr/Unsupervised-Rhythm-Clustering-Embedding/runs/CLMRv2-GTZAN-clustering/version_0/checkpoints/epoch=34-step=1645.ckpt" 

# learning options
num_clusters: 10
hidden_size: 32
alpha: 1

# audio data augmentation options
audio_length: 127840
sample_rate: 16000
transforms_polarity: 0.8
transforms_noise: 0.01
transforms_gain: 0.3
transforms_filters: 0.8
transforms_delay: 0.3
transforms_pitch: 0.6
transforms_reverb: 0.6
load_dataset: True
